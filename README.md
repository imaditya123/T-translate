# T-Translate: Transformer Model from Scratch âš¡

T-Translate is a **Transformer-based text generation model** built **from scratch** using PyTorch. It follows the original **Attention Is All You Need** paper, implementing a **custom Transformer architecture** for text-to-text tasks such as machine translation or text completion.

---

## **âœ¨ Features**  
âœ… Custom-built Transformer model (Encoder-Decoder) from scratch  
âœ… Uses PyTorch for efficient training and inference  
âœ… Supports text-to-text tasks like translation
âœ… Configurable hyperparameters via `config.py`  
âœ… Includes training, evaluation, and inference scripts  

---

## **ğŸ“‚ Project Structure**  
```
T-Translate/
â”‚â”€â”€ models/             # Saved trained models
â”‚â”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ config.py       # Hyperparameter settings
â”‚   â”œâ”€â”€ model.py        # Custom Transformer model
â”‚   â”œâ”€â”€ preprocess.py   # Data preprocessing utilities
â”‚   â”œâ”€â”€ train.py        # Training script
â”‚   â”œâ”€â”€ evaluate.py     # Model evaluation
â”‚   â”œâ”€â”€ translate.py    # Inference script
â”‚â”€â”€ notebooks/          # Jupyter notebooks for experiments
â”‚â”€â”€ tests/              # Unit tests for validation
â”‚â”€â”€ requirements.txt    # Dependencies
â”‚â”€â”€ README.md           # Documentation
â”‚â”€â”€ .gitignore          # Ignore unnecessary files
```

---

## **ğŸ”§ Setup & Installation**  

### **1ï¸âƒ£ Clone the Repository**  
```bash
git clone https://github.com/imaditya123/T-translate.git
cd T-Translate
```

### **2ï¸âƒ£ Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Train the Model**  
```bash
python src/train.py
```

### **4ï¸âƒ£ Run Inference**  
```bash
python src/translate.py --text "Hello, world!"
```

---

## **âš™ï¸ Configuration (Modify `config.py`)**  
You can adjust **hyperparameters** like:  
```python
MODEL_DIM = 512         # Transformer hidden size
NUM_LAYERS = 6          # Number of encoder & decoder layers
NUM_HEADS = 8           # Multi-head attention heads
DROPOUT = 0.1           # Dropout rate
BATCH_SIZE = 32         # Training batch size
LEARNING_RATE = 3e-4    # Optimizer learning rate
EPOCHS = 10             # Number of training epochs
```

---

## **ğŸš€ Future Enhancements**  
ğŸ”¹ Add support for sequence classification tasks  
ğŸ”¹ Optimize for large-scale datasets  
ğŸ”¹ Deploy as an API  

---

## **ğŸ“œ References**  
- Vaswani et al., **Attention Is All You Need** (2017)  
- PyTorch Documentation: https://pytorch.org/docs/stable/index.html  

ğŸ“¢ **Contributions & feedback are welcome!** Open an issue or create a pull request. ğŸš€

---

## License

This project is licensed under the MIT License - see the [LICENSE](https://github.com/imaditya123/T-translate?tab=MIT-1-ov-file) file for details.

---